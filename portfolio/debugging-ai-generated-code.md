# Debugging AIâ€‘Generated Code: Lessons Learned from Realâ€‘World Iteration

This case study captures the part of AIâ€‘augmented engineering that people rarely talk about:  
the debugging, the hallucinations, the lateâ€‘night fixes, the broken builds, and the persistence required to turn AIâ€‘generated code into a working system.

This wasnâ€™t a clean â€œAI wrote it and it just workedâ€ experience.  
It was a real engineering process â€” with AI as a collaborator, not a shortcut.

---

## ğŸ§  The Starting Point

After building my first AIâ€‘assisted project, I wanted to create a second one that would complement it.  
I had a working prototype of a small app (originally for tracking smoking, later converted to water intake), and I wanted to rebuild it cleanly using:

- A strong coder model  
- A clear prompt  
- A more focused architecture  
- A local development environment  
- A Docker deployment  

I expected some friction.  
I did not expect the amount of debugging that followed.

---

## âš ï¸ Where Things Went Off the Rails

### **1. Overly general prompts**
My early prompts were too openâ€‘ended:

- â€œPick what you think is bestâ€  
- â€œChoose the architecture you preferâ€  

This gave the model too much freedom, and it drifted quickly.

### **2. Hallucinated components**
The model invented:

- Files that didnâ€™t exist  
- Routes I never asked for  
- Variables that werenâ€™t defined  
- Entire flows that didnâ€™t match the project  

### **3. Incorrect assumptions**
The model assumed:

- Different frameworks  
- Different folder structures  
- Different database schemas  

### **4. Misleading debugging suggestions**
Some of the modelâ€™s â€œfixesâ€ made things worse:

- Wrong error explanations  
- Incorrect dependency advice  
- Fixes that introduced new bugs  

### **5. The build broke repeatedly**
I deployed the app to my local Docker server and watched it fail in real time.

This was the turning point.

---

## ğŸ› ï¸ Humanâ€‘inâ€‘theâ€‘Loop Debugging

This is where the project became valuable.

I:

- Reviewed every error manually  
- Forced the model to explain its reasoning  
- Overrode incorrect suggestions  
- Reâ€‘prompted with tighter constraints  
- Rebuilt the project stepâ€‘byâ€‘step  
- Used snapshots to preserve working states  
- Corrected hallucinations by hand  
- Reâ€‘tested every fix  

This wasnâ€™t â€œAI coding for me.â€  
This was **AIâ€‘augmented engineering**.

The model generated ideas.  
I validated them.  
The model wrote code.  
I corrected it.  
The model hallucinated.  
I grounded it.  
The model drifted.  
I steered it back.

This is the real workflow.

---

## ğŸ”„ The Breakthrough

After several nights of iteration, the app finally:

- Built cleanly  
- Ran inside Docker  
- Connected to the database  
- Responded to requests  
- Behaved consistently  

It wasnâ€™t perfect â€” but it worked.  
And it was mine.

---

## ğŸ§  What I Learned

### **1. AI is powerful, but not reliable without oversight**
It can accelerate development, but it cannot replace engineering judgment.

### **2. Prompt specificity is everything**
The more precise the prompt, the more stable the output.

### **3. Smaller models are often more predictable**
Large models hallucinate more aggressively.

### **4. Debugging AIâ€‘generated code is a real skill**
It requires:

- Pattern recognition  
- Patience  
- Understanding model behavior  
- Knowing when to override the AI  

### **5. Persistence matters more than perfection**
The final product wasnâ€™t the point.  
The process was.

---

## ğŸš€ What Iâ€™d Improve Next

- Add a dedicated **debugger agent**
